---
layout: post
title: Blog Post 5 - Fine Tuning Conditional Transformer Language Model (CTRL) for Controllable Generation
---

### [Alex Berry](https://www.linkedin.com/in/alexander-berry-569155122/), [Jason Chan](https://www.linkedin.com/in/jason-chun-sing-chan-803654174/), [Hyunjoon Lee](https://www.linkedin.com/in/hyunjoon-lee-a39883119/), [Sayan Samanta](https://www.linkedin.com/in/sayan-samanta-90592a23/), [Christina Ye](https://www.linkedin.com/in/yechristina/)
Brown University Data Science Initiative  
DATA 2040: Deep Learning  
May 14th, 2020


![CTRL Example](https://github.com/csjasonchan357/text-review-generation-data2040/raw/master/figures/ctrl.gif)
**Figure 1.** *A demonstration using the control code "Links" and a URL prompt*

### Overview

Along with PPLM, we attempted to improve our baseline model with CTRL. This 1.63 billion-parameter transformer language model is trained to condition on a "control code" that specifies the domain, entities, and relationships between entities in the generated text. Control codes are derived from structure that naturally occurs in raw text.

For our purposes, we used [this](https://arxiv.org/abs/1909.05858) pretrained CTRL model from Salesforce that allows for fine-tuning and the creation of new control codes. Our plan was to create control codes for different training sets with different sizes and determine which combination enabled for the most realistic review generation.

### Our Implementation

When attempting to fine-tune the Salesforce model for new control codes, we encountered two connected problems. The first was the model's failure to provide a prompt for the control code and starting input during the generation process. We eventually traced this back to the second problem, an error in the training process. Even with our smallest-sized data set, the training process quickly used up all of the GPU's memory (within 30 seconds). This problem persisted even when we used a pretrained control code for reviews and migrated the generation environment from Colab to Google Cloud Platform. Luckily, however, there exists a [lower-memory usage version](https://github.com/salesforce/ctrl/tree/lower_memory) of the model that we used instead.

We the pretrained "Reviews" control code with prompts for the review ratings:

#### Rating: 1

*This book is just another example of how to make money off of people who are gullible enough to believe in psychics. Rating: 1.0*

*I found this book to be poorly written and full of errors. Some sentences don't even make sense. Don't waste your time or money! Rating: 1.0*

*Not what I expected. Very short read. No real information provided. Would like more info on the subject. Rating: 1.0*

#### Rating: 5

*This item works as advertised. My wife uses it to listen to music on her phone while she works. Shes has had no problems with it, so far. Rating: 4.0.*

*Good quality sound from such small speakers. Easy to use and set up. Rating: 4.0*

*I love these. I can take them anywhere and put them wherever I want. And they don't cost much. Rating: 4.0*

It is interesting to note that in the case of the **Rating: 5** prompt, all of the generated reviews actually included ratings of 4.0. This might be due to an imbalanced lack of 5.0 ratings in the training set or just random variation.

### Outlook

CTRL's generated reviews were well-formed and stayed on topic, especially when compared to the more primitive results from the GPT-2 model.

Compared to the other models we researched, CTRL has the downside of not allowing as granular control of the generated review. This means that we can only specify the general domain of the review, not the actual entity mentioned (in this case, the product). For example, we can specify "clothing" as the domain and provide a starting input of "The belt is...", but we cannot ensure that "belt" will be the main entity described by the text. In addition, CTRL is a very large model and is difficult to train and host for generation on a local machine. We were only able to use the modified version in which all of the parameters are converted from 32-bit types to 16-bit types. It is therefore impractical to use for most everyday situations (i.e., it could not be easily embedded in a Dash web app).

To be more successful with CTRL in the future, we would investigate ways to introduce a notion of a specific entity into the CTRL generation process. One simple way to do this would be to create separate control codes for "BeltReview" and "ShirtReview" using separate training sets. This would require much more data than we currently have access to or the tools to manipulate efficiently.

### References

1. Keskar, Nitish Shirish, Bryan McCann, Lav Varshney, Caiming Xiong, and Richard Socher. “CTRL - A Conditional Transformer Language Model for Controllable Generation.” *GitHub. Salesforce*, April 20, 2020. https://github.com/salesforce/ctrl. 

(The authors provide both a technical basis for the CTRL approach to artificial text generation in their publication and a GitHub repository, containing the pretrained model which can be used as-is or fine-tuned with additional training. This source provided the low-memory, pretrained CTRL model and demonstration diagram at the top of this post.)

