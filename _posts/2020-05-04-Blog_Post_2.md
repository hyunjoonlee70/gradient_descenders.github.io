---
layout: post
title: Blog Post 2 - Exploratory Data Analysis (EDA)
---

### Alex Berry, Jason Chan, Hyunjoon Lee, Sayan Samanta, Christina Ye
Brown University Data Science Initiative  
DATA 2040: Deep Learning  
May 4th, 2020

## The Data Set

To recap from our blog post 1, the data we are using is a subset of the [“Amazon Review Data (2018)”](https://nijianmo.github.io/amazon/index.html) data set created by Jianmo Ni at UCSD. This larger data set is an updated version of the original Amazon review data set from 2014. It includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs). The 2018 version contains a total of 233.1 million reviews. Our project-specfic dataset is composed of reviews limited to the "Clothes, Shoes, and Jewelry" category, of which there are 11,285,464. The organization of the data set is one review per line in JSON format. A sample review might look like:

```
{
"image": ["https://images-na.ssl-images-amazon.com/images/I/71eG75FTJJL._SY88.jpg"],
"overall": 5.0, 
"vote": "2", 
"verified": True, 
"reviewTime": "01 1, 2018", 
"reviewerID": "AUI6WTTT0QZYS", 
"asin": "5120053084", 
"style": {
	"Size:": "Large", 
	"Color:": "Charcoal"
	}, 
"reviewerName": "Abbey", 
"reviewText": "I now have 4 of the 5 available colors of this shirt... ", 
"summary": "Comfy, flattering, discreet--highly recommended!", 
"unixReviewTime": 1514764800
}
```

## Preprocessing

Some preprocessing was necessary before we proceeded to our EDA. Firstly, we gave each review an ID and dropped all the features except rating and review text. This allowed us to rid of unnecessary information and shrink the size of the dataset considerably. Again to recap, our target in this project is to study the possibility of generating product reviews based on a given set of keywords. Therefore, we generated a keyword for each review text. For keyword generation, we used code from Ng Wai Foong's tutorial from ["this link"](https://medium.com/better-programming/extract-keywords-using-spacy-in-python-4a8415478fbf) which used NLP library called spaCy, developed at MIT. Lastly, we converted the file format from JSON to TSV. We use Tab-seperated values (TSV) format because it is very efficient for programming languages like Python and Tensorflow.


## Exploratory Data Analysis (EDA)
We used a randomly sampled subset of 278,646 samples. (ID, Rating, Keyword, Review Text).

```python
parent_directory = os.path.dirname(os.getcwd())
df = pd.read_csv(parent_directory+"/data/Clothing_Shoes_and_Jewelry_5.tsv", sep= "\t")
df.head()
```
![help](https://github.com/hyunjoonlee70/hyunjoonlee70.github.io/blob/master/images/df_head.png)

For EDA purpose, we also generated a feature called "polarity" using TextBlob. Polarity ranges from -1.0 to 1.0, where -1.0 means extremely negative and 1.0 means extremely positive, and 0.0 means neutral. We refered to Susan Li's tutorial from ["this link"](https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a).
```python
df['polarity'] = df['text'].map(lambda text: tb.TextBlob(str(text)).sentiment.polarity)
df['text'] = df['text'].astype("U")
```

### Distribution of Polarity and Reviews
The distribution of the sentiment polarity is close to normal distribution ranging from about -0.5 to 1.0. According to the distribution, people generally give neutral to moderately positive reviews to the products. From our plot, we have that our dataset is good representative sample with a wide range of reviews with regards to sentiment.
![help](https://github.com/csjasonchan357/text-review-generation-data2040/blob/master/figures/polarity_distribution.png)

The distribution of ratings is left-skewed (negatively skewed). We can see that people generally tend to giving 4 and 5 star reviews.
![help](https://github.com/csjasonchan357/text-review-generation-data2040/blob/master/figures/rating_distribution.png)

### Top Keywords
We looked at the top 20 keywords from our dataset.
![help](https://github.com/hyunjoonlee70/hyunjoonlee70.github.io/blob/master/images/top_keywords_counts.png)

We have that the top 20 keywords include positive adjectives like "great", "nice", "good", "comfortable" and products like "shoes (shoe)", "shirt", "bra". 


Next, we looked at the average ratings and average sentiment polarity of the keywords. The purpose of this exploration to examine whether the ratings fairly represents the sentiment polarity of the review text (at least have a reasonable association). According to our plots, we have that keywords "great", "nice", "good", "comfortable" have average ratings of above 4 and higher average sentiment polarities (above 0.3). We have for keyword "size", the average rating is below 4, which means that most people post reviews of size when they are generally not satisfied (size too big, size too small). The average sentiment polarity of keyword "size" was the only polarity below 0.2. Other keywords tell a similar story. The ratings represents the sentiment polarity of the reviews fairly well. 
![help](https://github.com/csjasonchan357/text-review-generation-data2040/blob/master/figures/top_keywords_avg_rating.png)
![help](https://github.com/csjasonchan357/text-review-generation-data2040/blob/master/figures/top_keywords_avg_polarity.png)


## Next Steps
In fact, creating a simple neural network model and iterating it is in the next steps in our competition project process. First we will preprocess the image data further. This includes de-noising it, adding a threshhold filter to it (to turn it from greyscale to black and white, essentially define the outlines of the written parts more clearly), contour it (make a bounding box and use only parts of the image that contain important information, essentially get rid of white space), and rescale it. From there, we will build a model with both convolutional layers, in order to extract feature information about the images, and then split the model into three dense layers which serve as producing the output classifications for the three components. We are looking forward to implementing all of this!

## References
